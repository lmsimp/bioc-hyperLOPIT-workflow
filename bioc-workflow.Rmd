---
title: "A Bioconductor Workflow for Processing and Analysing Spatial Proteomics Data"
author: "Lisa Breckels"
output: html_document
---

```{r env, echo=FALSE}
library("BiocStyle")
suppressPackageStartupMessages(library("MSnbase"))
suppressPackageStartupMessages(library("pRoloc"))
```

# Introduction

Quantitative mass spectrometry based spatial proteomics involves
elaborate, expensive and time consuming experimental procedures and
considerable effort is invested in the generation of such
data. Multiple research groups have described a variety of approaches
to establish high quality proteome-wide datasets. However, data
analysis is as critical as data production for reliable and insightful
biological interpretation. Here, we walk the reader through a typical
pipeline for the analysis of such data using several Bioconductor
packages for the R statistical programming environment.

The main package to analyse protein localisation data is
`r Biocpkg("pRoloc")`, which offers a set of dedicated functions for the
analysis of such data. `r Biocpkg("pRoloc")` itself relies on `r Biocpkg("MSnbase")` to
manipulate and process quantitative proteomics data. Many other
packages are used by `r Biocpkg("pRoloc")` for clustering, classification and
visualisation.

In this workflow, we will describe how to prepare the spatial
proteomics data starting from a spreadsheet containing quantitative
mass spectrometry data. We will focus on a recent pluripotent mouse
embryonic stem cells experiment [@hyper]. Additional annotated and
pre-formatted datasets from various species are readily available in
the `r Biocexptpkg("pRolocdata")` package.

Installation of Bioconductor package is documented in details on the
[Bioconductor installation help page](http://bioconductor.org/install/#install-bioconductor-packages). This
procedure is also applicable to any packages, from
[CRAN](https://cran.r-project.org/) as well as GitHub. Once a package
has been installed, it needs to be loaded for it functionality become
available in the R session; this is done with the `library` function.

If you have questions about this workflow in particular, or about
other Bioconductor packages in general, they a best asked on the
[Bioconductor support site](https://support.bioconductor.org/)
following the
[posting guidelines](http://www.bioconductor.org/help/support/posting-guide/). Questions
can be tagged with specific package names or keywords. For more
general information about mass spectrometry and proteomics, the
readers are invited to read the `r Biocexptpkg("RforProteomics")`
package vignettes and associated papers [@Gatto:2014;@Gatto2015]. 


# Reading and handling mass-spectrometry based proteomics data

## The use-case: prediction sub-cellular localisation in pluripotent embryonic mouse stem cells

As a use-case, we analyse a recent high-throughput spatial proteomics
dataset from pluripotent mouse embryonic stem cells (E14TG2a)
[@hyper]. The data was generated using hyperplexed LOPIT (hyperLOPIT),
an improved method relying on improved sub-cellular fractionation and
more acurate quantitation, leading to more reliable classification of
protein localisation across the whole sub-cellular space. The method
uses an elaborate sub-cellular fractionation scheme, enabled by the
use of Tandem Mass Tag (TMT) [Thompson:2003] 10-plex and application
of the MS data acquisition technique named synchronous precursor
selection MS$^3$ (SPS-MS$^3$) [@McAlister:2014], for TMT
quantification with high accuracy and precision. Three biological
replicates were generated from the E14TG2a experiment, the first was
to target low density fractions and the second and third were to
emphasis separation of the denser organelles.  The intersect of
replicates 1 and 2 was treated as a 20-plex dataset for the analysis
discussed in the manuscript [@hyper] as it has been shown that
combining replicates across from different gradients can increase
spatial resolution [@trotter]. The combination of replicates resulted
in 5032 proteins common in both experiments.

These, as well as many other data are directly available as properly
structured and annotated computational object from the 
`r Biocexptpkg("pRolocdata")` experiment package. In this workflow, we
will start with a description of how to generate these ad hoc objects
starting from any arbitrary spreadsheets, as produced by many popular
third-party applications. 

While we focus here on a LOPIT-type dataset, these analyses are
relevant for any quantitative spatial proteomics data, irrespective of
the fractionation or quantitation (i.e. labeled or label-free)
methods.

## The infrastructure: `r Biocpkg("pRoloc")` and `r Biocpkg("MSnbase")` in Bioconductor

To make use of the full functionality of the `r Biocpkg("pRoloc")`
software one needs to import their data into R and prepare them as an
`MSnSet`. The `MSnSet` is a dedicated data structure for the efficient
manipulation and processing of mass spectrometry and proteomics data
in R. Figure 1 illustrates a simplified view of the `MSnSet`
structure; there exists 3 key sub-parts (termed slots) to such a data
object: (1) the `exprs` slot for storing the quantitation data, (2)
the `fData` slot for storing the feature meta-data, and finally (3)
the `pData` slot for storing the sample meta-data.

![Simplified representation of the `MSnSet` data structure (reproduced with permission from the `r Biocpkg("MSnbase")` vignette)](./Figures/msnset.png)

There are a number of ways to import quantitation data and create an
`MSnSet` instance and all methods are described in the `r
Biocpkg("MSnbase")`
[input/output capabilities vignette](http://bioconductor.org/packages/release/bioc/vignettes/MSnbase/inst/doc/MSnbase-io.pdf). One
suggested simple method is to use the function `readMSnSet2` in 
`r Biocpkg("MSnbase")`. The function takes a single spreadsheet as input
and extracts the columns containing the quantitation data, as
identified by the argument `ecol`, to create the expression data,
while the other columns in the spreadsheet are appended to the feature
meta-data slot.  By example, in the code chunk below we read in the
`csv` spreadsheet containing the quantitation data from the intersect
of replicates 1 and 2 of the mouse map [@hyper], using the
`readMSnSet2` function. The data is as available online with the
manuscript (see tab 2 of the `xlsx` supplementary data set 1 in
[@hyper], which should be exported as a text-based spreadsheet) and
also as a `csv` in the Bioconductor `r Biocexptpkg("pRolocdata")` data
package.

To use the `readMSnSet2` function, as a minimum one must specify the
file path to the data and which columns of the spreadsheet contain
quantitation data. The `getEcols` function exists to help users
identify which columns of the spreadsheet contain the
quantitation data. 

We first locate the the spreadsheet of E14TG2a data distributed with
the `r Biocexptpkg("pRolocdata")` data using `system.file`, that
locates the `extdata` in the `r Biocexptpkg("pRolocdata")` package on
the hard drive and `dir`, that displays the full path to the file
matching the `hyperLOPIT-SIData-ms3-rep12-intersect.csv` pattern,
which corresponds the the file of interest. In the last line, we print
the filename (not the full path, which will vary from computer to
computer). 

```{r getFilename}
library("MSnbase")
extdatadir <- system.file("extdata", package = "pRolocdata")
csvfile <- dir(extdatadir, full.names = TRUE,
          pattern = "hyperLOPIT-SIData-ms3-rep12-intersect.csv")
basename(csvfile)
```

Note that the file is compressed (as indicated by the `gz`, for
`gzip`, extension), and will be decompressed on-the-fly when read into
R later on.

The spreadsheet that was deposited by the authors contains two
headers, with the second header containing information about where the
quantitation data is stored. 

![A screenshot of the data in the spreadsheet.](./Figures/spreadsheet-screenshot.png)

We can display the names of the second header by calling the
`getEcols` function with the argument `n = 2` (the default value is `n
= 1`), to specify that we wish to display the column names of the
second line.


```{r getEcols}
getEcols(csvfile, split = ",", n = 2)
```

It is now easy for one to identify that the quantitation data,
corresponding to the 10 TMT isobaric tags, is located in columns 8
to 27. We now have the two mandatory arguments to `readMSnSet2`,
namely the filename (stored in the `csvfile` variable) and the
quantitation column indices. In addition to these, it is also possible
to pass the optional argument `fnames` to indicate which column to use
as the labels by which to identify each protein in the sample. Here,
we use `fnames = 1` to use the Uniprot identifiers contained in the
first (unnamed) column of the spreadsheet. We also need to specify to
skip the first line of the file (for the same reason that we used 
`n = 2` in `getEcols` above) to read the `csv` data and convert it to an
`MSnSet` object.

```{r readMSnSet2}
lopit2016 <- readMSnSet2(csvfile, ecol = c(8:27), fnames = 1, skip = 1)
```

Below, we display a short summary of the data. The data contains 
`r nrow(lopit2016)` proteins/features common across the 2 biological replicates
for the respective 2 x 10-plex reporter tags (`r ncol(lopit2016)`
columns/samples), along with associated feature meta-data such as
protein markers, protein description, number of quantified peptides
etc (see below).


```{r showlopit2016}
lopit2016
```

As briefly mentioned above, the quantitation data is stored in the
`exprs` slot of the `MSnSet` and can be accessed by
`exprs(lopit2016)`. Below, we examine the quantitative information for
first 5 proteins. It is also possible to access specific rows and
columns by naming the proteins and fractions of interest.

```{r }
exprs(lopit2016)[1:5, ]
exprs(lopit2016)[c("Q9ERU9", "Q9Z2R6"), c("X126", "X131.1")]
```

The feature meta-data is stored in the `fData`
slot and can be accessed by `fData(lopit2016)`. When using
`readMSnSet2`, automatically, everything that is not defined as
quantitation data by `ecol` or the feature names by `fnames` is
deposited to the `fData` slot. As we wish to demonstrate the complete
analysis of this data we remove the results from the prior analysis
described in [@hyper] in the code chunk below. We see the `fData`
contains 25 columns describing information such as the number of
peptides, associated markers, machine learning results etc. 

For demonstration in the code chunk below keep the 2nd, 8th and 11th
columns which contain the Uniprot entry names and two different marker
sets to use an input for machine learning analyses (see sections on
markers and subsequent sections) and rename the first feature variable
names.

```{r removefData}
fvarLabels(lopit2016)
fData(lopit2016) <- fData(lopit2016)[, c(2, 8, 11)]
fvarLabels(lopit2016)[1] <- "UniProtId"
head(fData(lopit2016))
```

Note that when using the simple `readMSnSet2` procedure, sample
meta-data is kept empty. It is advised to annotate the fractions as
well. Below, we annotate the replicate from which the profiles
originate and the TMT tag (extracted from the sample/fraction names).

```{r pdata}
pData(lopit2016)$replicate <- rep(1:2, each = 10)
pData(lopit2016)$tag <- sub("\\.1$", "", sub("^X", "", sampleNames(lopit2016)))
pData(lopit2016)
```

## Data processing

### Normalisation

Before combination, the two replicates were separately normalised by
sum across the 10 channels (i.e. such that the sum of each protein's
intensity is 1), for each replicate respectively. Normalisation is an
essential part of data processing and several methods are available in
`r Biocpkg("MSnbase")`. The normalisation desired in this specific
case would be obtained with the a call to the `normalise` method.

### Missing data 


### Combining acquisistions


The combination of data can also be performed effectively in 
`r Biocpkg("MSnbase")` as detailed in the dedicated *Combining MSnSet
instances* section of the `r Biocpkg("MSnbase")`
[tutorial vignette](http://bioconductor.org/packages/release/bioc/vignettes/MSnbase/inst/doc/MSnbase-demo.pdf).


# Quality Control

Data quality is routinely examined through visualisation to verify that sub-cellular niches have been separated along the gradient. Based on De Duve's principle [@DeDuve:1981] proteins that co-localise exhibit similar quantitation profiles across the gradient fractions employed. One approach that has been widely used to visualise and inspect high throughput mass spectrometry-based proteomics data is principal components analysis (PCA). PCA is one of many dimensionality reduction methods, that allow one to effectively summarise multi-dimensional data in to 2 or 3 dimensions to enable visualisation. Very generally, the original continuous multi-dimensional data is transformed into a set of orthogonal components ordered according to the amount of variability that they describe. The `plot2D` method in `r Biocpkg("pRoloc")` allows one to plot the principal components (PCs) of a dataset against one another, by default the first two components are plotted on the x- and y-axis, respectively (the `dims` argument can be used to plot other PCs). If distinct clusters are observed, we assume that there is organellar separation present in the data. In the code chunk below we produce a PCA plot of the mouse stem cell dataset. One point on the plot represents one protein. We can indeed see several distinct protein clusters. Although, plotting the PCs does not give us a hard quantitative measure of separation, it is extremely useful summarising complex experimental information in one figure, to get an simplified overview of the data. 

```{r qcplot, message=FALSE, warning=FALSE}
library('pRoloc')
plot2D(lopit2016, fcol = NULL)
```

# Markers

In the context of spatial proteomics, a marker protein is defined as a well-known resident of a specific sub-cellular niche in a species *and* condition of interest. Applying this to machine learning (ML), and specifically supervised learning, for the task of protein localisation prediction, markers constitute the labelled training data to use as input to a classification analyses. Defining well-known residents, and obtaining labelled training data for ML analyses can be time consuming, but it is important to define markers that are representative of the multivariate data space and on which a classifier will be trained and generated. `r Biocpkg("pRoloc")` provides a convenience function, `addMarkers`, to directly add markers to a `MSnSet` object, as demonstrated in the code chunk below. These marker sets can be accessed using the `pRolocmarkers()` function. Marker sets are stored as a simple named vector in R, and originate from in-house user-defined spreadsheets or a set of markers from previous published studies. The marker vectors that can be accessed from `pRolocmarkers` are named vectors and to enable mapping between the markers and the `MSnSet` instance it is required that the `featureNames` of the `MSnSet` instance match the `names` of the marker. The mouse dataset used here has Uniprot IDs stored as the `featureNames` (see `head(featureNames(lopit2016))`) and the names of the vector of the mouse markers (`mmus` markers) are also Uniprot IDs (see `head(mrk)` in the code chunk below), so it is straightforward to match names between the markers and the `MSnSet` instance. If the naming between the marker sets and the `MSnSet` dataset are different, one will have to convert and match the proteins according to the appropriate identifier. Sometimes, we find the equivalent entry name, Uniprot ID or accession number is stored with the data, which makes conversion between identifers relatively straightforward. If this is not the case however, there are conversion softwares online available, for example XXX.   

In the code chunk below, we demonstrate how to add markers using `pRolocmarkers` function and then visualise these annotations using the `plot2D` function.

```{r addmrkers}
## List available marker sets
pRolocmarkers()

## Use mouse markers
mrk <- pRolocmarkers(species = "mmus")
head(mrk)

## Add mouse markers
lopit2016 <- addMarkers(lopit2016, mrk)

## Plot mouse markers
plot2D(lopit2016, fcol = "markers", main = "pRolocmarkers for mouse")
```

In general, the Gene Ontology (GO) [@Ashburner:2000], and in particular the cellular compartment (CC) namespace are a good starting point for protein annotation and marker definition. It is important to note however that automatic retrieval of sub-cellular localisation information, from `r Biocpkg("pRoloc")` or elsewhere, is only the beginning in defining a marker set for downstream analyses. Expert curation is vital to check that any annotation added is in the correct context for the the biological question under investigation. 

# Interactive visualisation



# Novelty Detection

The extraction of sub-cellular protein clusters can be difficult owing to the limited number of marker proteins that exist in databases and elsewhere. Furthermore, given the vast complexity of the cell, automatic annotation retrieval does not always give a full representation of the true sub-cellular diversity in the data. For downstream analyses, such as supervised machine learning, it is desirable to obtain reliable markers that cover as many sub-cellular niches as possible, as these markers are directly used in the training phase of the ML classification. We find that a lack of sub-cellular diversity in the labelled training data leads to prediction errors, as unlabelled instances can only be assigned to a class that exists in the training data [@Breckels:2013]. In such scenarios novelty detection can be useful to identify data-specific sub-cellular groupings such as organelles and protein complexes. The phenotype discovery (phenoDisco) algorithm [@Breckels:2013] is one such method and is available in `r Biocpkg("pRoloc")`. It is an iterative semi-supervised learning method that combines the classification of proteins on existing labelled data with the detection of new clusters. Novelty detection methods are also useful for confirming the presence of known clusters in an unbiased fashion. For example, in [@hyper] the `phenoDisco` algorithm was used to detect the data-specific confirmation and presence of the nucleus and nucleus sub-compartments, as it was expected that nucleus associated clusters existed within the data as the protocol included a separate chromatin-rich fraction to enrich for nuclei. In the code chunk below, we show how to run a series of novelty detection experiments using the `phenoDisco` algorithm on the mouse stem-cell dataset. There are several optional arguments that can be set dependent on the type of biological question of interest. For example, 

- Explain how to use different group sizes for detecting small complexes etc.
- ndims for using >=2 components
- times -> important for convergence
- input markers are different here -> explain choice of pd.markers

```{r pd, eval = FALSE}
## As per the hyperLOPIT paper
pdRes <- phenoDisco(lopit2016, fcol = "phenoDisco.Input", times = 200, 
                    GS = 20, p = 0.05)

```

# Supervised machine learning

Supervised machine learning, also known as classification, is an essential tool for the assignment of proteins to distinct sub-cellular niches. Using a set of labelled training examples i.e. markers, we can train a machine learning classifier to learn a mapping between the data i.e. the quantitative protein profiles, and a known localisation. The trained classifier can then be used to predict the localisation of a protein of unknown localisation, based on its observed protein profile. To date, this method has been extensively used in spatial quantitative proteomics to assign thousands of proteins to distinct sub-cellular niches [@hyper; @Groen:2014; @trotter; @Hall:2009; @Dunkley:2006; @Tan:2009]. 

There are several classification algorithms readily available in `pRoloc`, which are documented in the dedicated [`pRoloc` machine learning techniques vignette](http://bioconductor.org/packages/release/bioc/vignettes/pRoloc/inst/doc/pRoloc-ml.pdf). We find the general tendancy to be that it is not the choice of classifier, but the improper optimisation of the algorithmic parameters, that limits classification accuracy. Before employing any classification algorithm and generating a model on the training data, one must first find the optimal parameters for the algorithm of choice. 

## Optimisation

In the code chunk below we employ the use of a Support Vector Machine (SVM) to learn a classifier on the labelled training data. The training data is found in the `featureData` slot in the column called `SVM.marker.set`. As previously mentioned one first needs to train the classifiers parameters before an algorithm can be used to predict the class labels of the proteins with unknown location. One of the most common ways to optimise the parameters of a classifier is to partition the labelled data in to training and testing subsets. In this framework parameters are tested via a grid search using cross-validation on the training partition. The best parameters chosen from the cross-validation stage are then used to build a classifier to predict the class labels of the protein profiles on the test partition. Observed and expected classication results can be compared, and then used to assess how well a given model works by getting an estimate of the classiers ability to achieve a good generalisation i.e. that is given an unknown example predict its class label with high accuracy. In `pRoloc` algorithmic performance is estimated using stratified 80/20 partitioning for the training/testing subsets respectively, in conjuction with five-fold cross-validation in order to optimise the free parameters via a grid search. This procedure is usually repeated 100 times and then the best parameter(s) are selected upon investigation of classifier accuracy, here we use the harmonic mean of precision and recall; the macro F1 score. In the code chunk below we demonstrate how to optimise the free parameters; `sigma` and `cost`, of a classical SVM classifier with a Gaussian kernel using the function `svmOptimisation`. As the number of labelled instances per class varies from organelle to organelle, we can account for class imbalance by setting specific class weights when generating the SVM model. Below the weights, `w` are set to be inversely proportional to the class frequencies. 

```{r loadpdRes, include=FALSE}
pdRes <- lopit2016
load("Data/fusionparams.rda")
```
```{r optimise, eval=FALSE}
(w <- table(fData(pdRes)[, "SVM.marker.set"]))
w <- 1/w[names(w) != "unknown"]

## 100 rounds of optimisation with five-fold cross-validation
params <- svmOptimisation(pdRes, fcol = "SVM.marker.set",
                          times = 100, xval = 5,
                          class.weights = w,
                          verbose = FALSE)
```

The output `params` is an object of class `GenRegRes`; a dedicated container for the storage of the design and results from a machine learning optimisation. To assess classifier performance we can examine the macro F1 scores and the most frequently chosen parameters. A high macro F1 score indicates that the marker proteins in the test dataset are consistently and correctly assigned by the the algorithm. Often more than one parameter or set of parameters gives rise to the best generalisation accuracy. As such it is always important to investigate the model parameters and critically assess the best choice. The best choice may not be as simple as the parameter set that gives rise to the highest macro F1 score and one must be careful to avoid overfitting and to choose parameters wisely. 

```{r visualiseOpt, eval=FALSE}
(best <- getParams(params))
plot(params)
levelPlot(params)
```
```{r visualiseOptHide, include=FALSE}
grid.arrange(plot(params), levelPlot(params), ncol=2)
```

By using the function `getParams` we can extract the best set of parameters. Currently, `getParams` retrieves the first best set is automatically but users are encouraged to critically assess whether this is the most wise choice by visualising the results using the methods `plot` and `levelPlot`. The `plot` method for `GenRegRes` object shows the respective distributions of the 100 macro F1 scores for the best cost/sigma parameter pairs, and `levelPlot` shows the averaged macro F1 scores, for the full range of parameter values. Once we have selected the best parameters we can then use them to build a classifier from the labelled marker proteins. 

## Classification

We can use the function `svmClassification` to return a classification result for all unlabelled instances in the dataset corresponding to their most likely sub-cellular location. The algorithm parameters are passed to the function, along with the class weights and the `fcol` to tell the function where the labelled training data is located, in our case, our marker proteins are located in the `featureData` with the label `"SVM.marker.set"`.

```{r classify, eval=TRUE}
svmRes <- svmClassification(pdRes, params,
                            class.weights = w,
                            fcol = "SVM.marker.set")
```

Automatically, the output of the above classification; the organelle predictions and assignment scores, are stored in the `featureData` slot of the `MSnSet`. In this case, they are given the labels `svm` and `svm.scores` for the predictions and scores respectively. The resultant predictions can be visualised using `plot2D`. In the code chunk below `plot2D` is called to generate a PCA plot of the data and `fcol` is used to specify where the new assignments are located, here for example these are located in the column called `svm`. Additionally, when calling `plot2D` we use the `cex` argument to change the point size of each point on the plot (where one point represents one protein) to be inversely proportional to the SVM score. This gives an initial overview of the high scoring localisations from the SVM predictions.

```{r setColors, include=FALSE}
setStockcol(paste0(getStockcol(), 70))
```
```{r plotSVM, eval=TRUE}
ptsze <- exp(fData(svmRes)$svm.scores) - 1
plot2D(svmRes, fcol = "svm", cex = ptsze)
addLegend(svmRes, fcol = "svm", where = "bottomleft", bty = "n", cex = .5)
```

## Thresholding


## Transfer learning

# Session information

The function `sessionInfo` provides a summary of all packages and
versions used to generate this document. This enables us to record the
exact state of our session that lead to these exact
results. Conversely, if the script stops working of if it returns
different results, we are in a position to re-generate the original
results using the adequate software versions and retrace changes in
the software that lead to failure and/or different results.

```{r si}
sessionInfo()
```

It is always important to include session information details along
with a
[short reproducible example](http://adv-r.had.co.nz/Reproducibility.html)
highlighting the problem or
[question](https://support.bioconductor.org/) at hand.
